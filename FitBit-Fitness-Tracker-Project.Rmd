---
title: "FitBit-Fitness-Tracker-Project"
author: "PS"
date: '2023-03-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## About the Project

**Business Task:** This project aims to gain insights into smart device usage trends and their potential application to Bellabeat customers and use these findings to inform Bellabeat's marketing strategy. The analysis considers the following questions:

-   What are some trends in smart device usage?

-   How could these trends apply to Bellabeat customers?

-   How could these trends help influence Bellabeat marketing strategy

## About the Company

**Urška Sršen** and **Sando Mur** founded Bellabeat, a high-tech company that manufactures health-focused smart products. Collecting data on activity, sleep, stress, and reproductive health has allowed Bellabeat to empower women with knowledge about their own health and habits. Since it was founded in 2013, Bellabeat has grown rapidly and quickly positioned itself as a tech-driven wellness company for women.

By 2016, Bellabeat had opened offices around the world and launched multiple products. Bellabeat's products became available through a growing number of online retailers in addition to their own e-commerce channel on their [website](https://bellabeat.com/).

**Sršen** knows that an analysis of Bellabeat's available consumer data would reveal more opportunities for growth. She has asked the marketing analytics team to focus on a Bellabeat product and analyze smart device usage data in order to gain insight into how people are already using their smart devices. Then, using this information, she would like high-level recommendations for how these trends can inform Bellabeat's marketing strategy.

## About the Data

**Data Used**: The data set consisted of public data that explores smart device users' daily habits.

-   [FitBit Fitness Tracker Data](https://www.kaggle.com/datasets/arashnic/fitbit) (CC0: Public Domain, dataset made available through [Mobius](https://www.kaggle.com/arashnic)): This Kaggle data set contains personal fitness tracker from thirty Fitbit users. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring. It includes information about daily activity, steps, and heart rate that can be used to explore users' habits.

**Data Collection:** This dataset was generated by respondents to a distributed **survey** via Amazon Mechanical Turk between 03.12.2016-05.12.2016. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring.

-   **Second-Party Data**: [Amazon Mechanical Turk (MTurk)](https://www.mturk.com/) is a crowd-sourcing marketplace that makes it easier for individuals and businesses to outsource their processes and jobs to a distributed workforce who can perform these tasks virtually. This enables organizations to post tasks or surveys and pay people to complete them. Since the survey was subject to privacy regulations and data protection laws, the assumption that the data is second-party is made. **Second-party data** refers to data that is collected by one organization or entity and then shared with another organization or entity.
-   **Potential Selection Bias**: This bias occurs when individuals choose whether or not to participate in a study or survey, and this choice is related to the outcome being studied. In this case, since 30 users consented to track their data, those individuals may differ in important ways from those who do not consent to participate and may not be a representative sample of the entire population.

**Data Organization**: The entire data consisted of 18 tables of CSV files. Individual reports can be parsed by export session ID or timestamp. Variation between output represents the use of different types of Fitbit trackers and individual tracking behaviors / preferences.

-   Every file has an ID column, which is a unique identifier for each user. We can use this variable to identify how many users are considered in each table.

-   I defined a function in order to get the number of users, variables and data types of each column within each table. The variable that represents the `datestamp` is parsed as `character`. This can cause issues when working with dates; therefore, converting the data type from `character` to `Date` is required. Only the table `weightLogInfo` has a column with `logicals`. You may refer to the function in the documentation for further details.

-   The minimum first date in the `ActivityDate` is April 12th, 2016, while the last is May 12th of the same year. This provides a 30-day reporting period to analyze. Users' behavior during these 30 days may be influenced by factors that are specific to that period, such as a holiday or a special event. Such a small period might not be enough to generalize the users' behaviour in the long run; however, it can still provide valuable insights during that time.

| Table Name              | Type     | Participants | Description                                            |
|:-----------------|:-----------------|:-----------------|:------------------|
| dailyActivity           | CSV File | 33 users     | 15 variables: character 01 \| numeric 14               |
| dailyCalories           | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| dailyIntensities        | CSV File | 33 users     | 10 variables: character 01 \| numeric 09               |
| dailySteps              | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| heartrate               | CSV File | 14 users     | 03 variables: character 01 \| numeric 02               |
| hourlyCalories          | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| hourlyIntensities       | CSV File | 33 users     | 04 variables: character 01 \| numeric 03               |
| hourlySteps             | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minuteCaloriesNarrow    | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minuteCaloriesWide      | CSV File | 33 users     | 62 variables: character 01 \| numeric 61               |
| minuteIntensitiesNarrow | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minuteIntensitiesWide   | CSV File | 33 users     | 62 variables: character 01 \| numeric 61               |
| minuteMETsNarrow        | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minutesSleep            | CSV File | 24 users     | 04 variables: character 01 \| numeric 03               |
| minutesStepsNarrow      | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minutesStepsWide        | CSV File | 33 users     | 62 variables: character 01 \| numeric 61               |
| sleepDay                | CSV File | 24 users     | 05 variables: character 01 \| numeric 04               |
| weightLogInfo           | CSV File | 08 users     | 08 variables: character 01 \| numeric 06 \| logical 01 |

------------------------------------------------------------------------

# Process

I will focus on conducting the analysis by utilizing the programming language R. R has a vast collection of packages and libraries that provide powerful and flexible tools for data analysis, visualization, and modeling, and besides that, is free open-source software. I will be using some of the following packages:

-   `ggplot2`: A package for creating visually appealing and informative data visualizations. It is based on the grammar of graphics, which allows users to build complex plots by combining simple components.

-   `readr`: A package for reading in and parsing flat files (e.g. CSV files) into R data frames. It is designed to be fast and memory-efficient, making it ideal for working with large data sets.

-   `dplyr`: A package for data manipulation and transformation. It provides a set of easy-to-use functions for filtering, grouping, summarizing, and joining data, making it a powerful tool for data wrangling tasks.

-   `lubridate`: A package that provides functions to work with dates and times. It makes it easier to parse, manipulate, and format date-time objects in R. The package provides a consistent and user-friendly syntax for common date-time operations, which can be difficult to perform in base R.

-   `janitor`: The janitor package is an R package that provides a set of functions for data cleaning and data tidying tasks. It offers a range of simple and consistent functions that can help clean up messy datasets and quickly perform common data manipulation tasks.

```{r Load libraries, message = FALSE, warning = FALSE}
# Load libraries
library(ggplot2)
library(readr)
library(dplyr)
library(lubridate)
library(janitor)
library(tidyr)
library(cowplot)
```

As mentioned before, the entire data consisted of 18 tables of CSV files. The task of importing the data into R using `read_csv` could be cumbersome if we proceeded manually. Instead, I will create a function to do that task.

-   This function will import a list of CSV files located in a specified folder and returns them as a named list of data frames in R.

-   The original table names contain an underscore (`_`) somewhere in the string. I am going to use that underscore to create shorter names for each table in such a way that: `dailyActivity_merged.csv` will be `dailyActivity`, for instance.

-   In the end, this function will be useful for importing multiple CSV files into R as a single named list of data frames and, I will use that list to analyze their data type. The advantage of having a list of data frames instead of individual data frames is that it facilitates the management of multiple datasets. Rather than keeping track of individual data frames separately, a list can store and manipulate them as a single object. This can simplify the code and make it easier to apply functions or consistently manipulate the data across all datasets.

```{r Import data sets, message = FALSE}
# Create a function to import all the data sets within the folder as a list
importDataList <- function(folder_path = "data") {
        
        # Get a list off all files names in the given folder
        file_names <- list.files(path = folder_path)
        
        # Create empty vectors to store data names and data lists
        data_name <- c()
        data_list <- list()
        
        # Loop through each file in the list of file names
        for(i in seq_along(file_names)) {
                
                # Find the position of the first underscore in the file 
                char_pos <- sapply(lapply(strsplit(file_names, split = ""), 
                                          function(x) {which(x == "_")}), 
                                   function(x) {x[1]})
                
                # Extract the data name from the file name 
                data_name[i] <- substr(file_names[i], start = 1, stop = char_pos[i] - 1)
                
                # Read in the CSV file and store it in the data list
                file_path <- paste0(folder_path, "/", file_names[i])
                data_list[[i]] <- read_csv(file_path)
        }
        
        # Rename the data sets in the list and return the list
        names(data_list) <- data_name
        return(data_list)
}
bellaFit <- importDataList()
```

Once the data set is loaded, I need to change some variables.

1.  The variables that represent the `datestamp` are parsed as `character`. Therefore, for converting the data type from character to Date I use the following chuck of code. For instance, in `dailyActivity` data frame, the column is modified using the existing `ActivityDate` column, but with its format converted to date using the `lubridate::as_date` function. The specified format for the date conversion is `"%m/%d/%Y"`, which means that the original date strings are expected to be in the format of `month/day/year`.

```{r Parsing character as date}
# Change data type from text to date in dailyActivity data set
bellaFit$dailyActivity <- bellaFit$dailyActivity %>%
        mutate(ActivityDate = as_date(ActivityDate, format = "%m/%d/%Y"))

# Change data type from text to date in dailySteps data set
bellaFit$dailySteps <- bellaFit$dailySteps %>%
        mutate(ActivityDay = as_date(ActivityDay, format = "%m/%d/%Y"))

# Change data type from text to date in sleepDay data set
bellaFit$sleepDay <- bellaFit$sleepDay %>%
        mutate(SleepDay = mdy_hms(bellaFit$sleepDay$SleepDay))
```

2.  The column names also need to be modified. I use **`clean_names()`** function from the **`janitor`** package to clean up the column names of a data frame. In this code, the **`clean_names()`** is used to standardize the column names of three data frames within the **`bellaFit`** object: **`dailyActivity`**, **`dailySteps`**, and **`sleepDay`**.
3.  For the sake of consistency, we rename the data sets with this standardize convention as well.

```{r Rename column names}
# Clean up the column names of the dailyActivity data frame 
bellaFit$dailyActivity <- clean_names(bellaFit$dailyActivity)

# Clean up the column names of the dailySteps data frame
bellaFit$dailySteps <- clean_names(bellaFit$dailySteps)

# Clean up the column names of the sleepDay data frame 
bellaFit$sleepDay <- clean_names(bellaFit$sleepDay)

# Rename data set names
chosen_tables <- c("dailyActivity", "dailySteps", "sleepDay")
names(bellaFit)[which(names(bellaFit) %in% chosen_tables)] <- c("daily_activity", "daily_steps", "sleep_day")   
```

4.  We need to ensure that there are no duplicate rows in the data. Removing duplicated rows is an important step in data cleaning because duplicated data can bias statistical analysis and lead to incorrect results. Then I removed the duplicated rows afterward.

```{r Remove duplicated rows}
# Create a data frame that lists the number of duplicated rows in three tables of a larger data set
data.frame(
        
        # Column that lists the name of each table
        table_name = chosen_tables,
        
        # Column that lists the number of duplicated rows in each table
        duplicated_rows = c(
                
                # Calculate the number of duplicated rows
                sum(duplicated(bellaFit$daily_activity)), # daily_activity table
                sum(duplicated(bellaFit$daily_steps)), # daily_steps table
                sum(duplicated(bellaFit$sleep_day)) # sleep_day table
        )
) 

# Remote duplicated rows
bellaFit$sleep_day <- bellaFit$sleep_day %>% distinct()
```

# Analyze

1.  First, I would like to analyze the users' sleep schedules. Analyzing sleep duration during weekdays and weekends can help identify whether users are getting sufficient sleep or, on the other hand, whether they are oversleeping. First, I select the `total_minutes_asleep` and `total_time_in_bed` and then calculate summary statistics for those columns using the `summary()` function.

```{r Stats of sleeping habits per day type}
bellaFit$sleep_day %>% 
        select(total_minutes_asleep, total_time_in_bed) %>% 
        summary()
```

Calculating the mean and the median of `total_minutes_asleep` and `total_time_in_bed` can provide different types of information about the data in that column. Here are some inferences from these calculations:

-   Comparing both the **mean** and **median** can provide information about the center of the data and the presence of outliers. If the mean and median are close, it suggests that the data is fairly symmetrical and there are no significant outliers. Otherwise, if there is a significant difference between them, it suggests that the data may be skewed and there may be outliers that are affecting the mean.

-   In order to visualize the distribution of `total_minutes_asleep` and `total_time_in_bed` I will use box plots. I am interested in knowing if there is a difference between users' sleep schedules during weekdays, weekends and per day.

```{r Plotting sleeping habits per day type, fig.width = 10}
# Select the "sleep_day" column from the "bellaFit" data frame and remove the "total_sleep_records" column
sl1 <- bellaFit$sleep_day %>% select(-total_sleep_records) %>%
        
        # Create a new column "type_of_day" based on whether the day is a weekday or a weekend
        mutate(type_of_day = ifelse(wday(sleep_day) %in% 2:6, "Weekday", "Weekend")) %>%
        
        # Create a box plot of the "total_minutes_asleep" variable grouped by the "type_of_day" variable
        ggplot(mapping = aes(x = type_of_day, y = total_minutes_asleep)) +
        geom_boxplot() +
        
        # Add a horizontal line representing the mean of "total_minutes_asleep" across all days
        geom_hline(yintercept = mean(bellaFit$sleep_day$total_minutes_asleep), lty = "twodash", col = "red") +
        
        # Set the limits of the y-axis to be between 0 and 1000.
        scale_y_continuous(limits = c(0, 1000)) +
        
        # Customize the plot 
        theme(legend.position = "none") +
        labs(x = "", y = "Minutes Asleep")

# We create a box-plot as before, but using "total minutes in bed" on y-axis
sl2 <- bellaFit$sleep_day %>% 
        select(-total_sleep_records) %>%
        mutate(type_of_day = ifelse(wday(sleep_day) %in% 2:6, "Weekday", "Weekend")) %>%
        ggplot(mapping = aes(x = type_of_day, y = total_time_in_bed)) +
        geom_boxplot() +
        geom_hline(yintercept = mean(bellaFit$sleep_day$total_time_in_bed), lty = "twodash", col = "red") + 
        scale_y_continuous(limits = c(0, 1000)) +
        theme(legend.position = "none") +
        labs(x = "", 
             y = "Minutes in Bed")

# Arrange sl1 and sl2 into a grid
plot_grid(sl1, sl2, labels = c('A', 'B'), label_size = 15)
```

**Observations:**

-   Both variables `total_minutes_asleep` **(Plot A)** and `total_time_in_bed` **(plot B)** look similar, which suggests that are correlated and the difference of using one variable or the other might not be significant.

-   The double dash red line represents the global **mean** of both variables, while the center of the box represents the **median**. Therefore, during weekdays the **mean** and the **median** are close in value in both `total_minutes_asleep` and `total_time_in_bed`; however, during weekends the **median** is larger than the **mean**, which might be caused by some extremely low values in the data that are pulling the mean down.

```{r Plotting sleeping habits per day, fig.width = 10}
 # Select the "sleep_day" column from the "bellaFit" data frame and remove the "total_sleep_records" column 
sl3 <- bellaFit$sleep_day %>% select(-total_sleep_records) %>%
        
        # create a new column "day" with abbreviated day names as factor levels ordered chronologically
        mutate(day = factor(wday(sleep_day), 
                            labels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"),
                            ordered = TRUE)) %>%
        
        # Create a box plot of the "total_minutes_asleep" variable grouped by the "day" variable
        ggplot(mapping = aes(x = day, y = total_minutes_asleep)) +
        geom_boxplot() + 
        
        # Add a horizontal line representing the mean of "total_minutes_asleep" across all days in the "bellaFit" data frame. 
        geom_hline(yintercept = mean(bellaFit$sleep_day$total_minutes_asleep), lty = "twodash", col = "red") +
        
        # Set the limits of the y-axis to be between 0 and 1000.
        scale_y_continuous(limits = c(0, 1000)) +
        
        # Customize the plot 
        theme(legend.position = "none") +
        labs(x = "", y = "Minutes Asleep")

# We create a box-plot as before, but using "total minutes in bed" on y-axis
sl4 <- bellaFit$sleep_day %>% select(-total_sleep_records) %>%
        mutate(day = factor(wday(sleep_day), 
                            labels = c("Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"),
                            ordered = TRUE)) %>%
        ggplot(mapping = aes(x = day, y = total_time_in_bed)) +
        geom_boxplot() + 
        geom_hline(yintercept = mean(bellaFit$sleep_day$total_time_in_bed), lty = "twodash", col = "red") + 
        scale_y_continuous(limits = c(0, 1000)) +
        theme(legend.position = "none") +
        labs(x = "", 
             y = "Minutes in Bed")   

# Arrange sl3 and sl4 into a grid
plot_grid(sl3, sl4, labels = c('A', 'B'), label_size = 15)
```

**Observations:**

-   A similar analysis is conducted, but this time considering individual days. Again, both variables `total_minutes_asleep` **(Plot A)** and `total_time_in_bed` **(plot B)** look similar, which suggests that are correlated and the difference of using one variable or the other might not be significant.

-   It appears that there is no significant difference between the hours of sleep according to the day of the week from Monday to Saturday; nevertheless, on Sundays there is an increase in `total_minutes_asleep` **(Plot A)** and `total_time_in_bed,`which is not surprising.

2.  I am interested in how some variables affect the calories count. For this we can plot a correlation matrix and then a scatter plot to visualize the possible relationship between the variables.

-   A correlation matrix is a matrix that shows the correlation coefficients between several variables. In statistics, correlation refers to the degree of linear association between two or more variables.

-   For the Pearson correlation, an absolute value of 1 indicates a perfect linear relationship. A correlation close to 0 indicates no linear relationship between the variables.

-   A correlation matrix typically displays the correlation coefficients for all pairs of variables in a table format as follows.

```{r Correlation matrix}
# Select some variables that might be correlated to calories
bellaFit$daily_activity %>% 
        select(calories, 
               total_steps, 
               total_distance, 
               sedentary_minutes, 
               lightly_active_minutes, 
               fairly_active_minutes,
               very_active_minutes) %>% 

# In this part I rename the column to aesthetic  factors
        rename(cal = calories,
               ste = total_steps,
               dis = total_distance,
               sed = sedentary_minutes,
               lig = lightly_active_minutes,
               fai = fairly_active_minutes,
               ver = very_active_minutes) %>%

# Plot the correlation matrix
cor() %>% corrplot::corrplot(type = "lower", method = "number")

```

**Comments**:

-   In this result, we can see mostly positive linear relationships, except for `sendentary_minutes`, which has a weak negative relationship with `calories`.

-   We are interested in looking for a strong linear relationship between `calories` and the remaining variables. In the correlation matrix we can see that three variables: `total_steps` (0.59), `total_distance` (0.64) and `very_active_minutes` (0.62).

-   On the other hand, **calories** is known as a *dependent* variables, while the remaining variables are *independent*. There must not be any relationship between independent variables. In other words, the correlation coefficient must be as close to zero as possible. Here `total_distance` and `total_steps` are highly correlated.

-   In linear regression model this is called *collinearity*. It can cause problems in statistical analyses because it can lead to unstable and unreliable estimates of the regression coefficients. This is because when two or more predictor variables are highly correlated, it becomes difficult to determine the unique contribution of each variable to the outcome variable.

-   In the scatterplot below we can see `calories` and `total_distance`. Looking for the overall pattern we determine that there is indeed a positive linear relationship, but there is no complete pattern between the two variables. In addition, there are points that are far away from the others, which may indicate unusual values that affect the tendency of the tendency.

```{r Scatterplot calories-total_distance}
# Selecting the columns "calories" and "total_distance" from the data frame bellaFit$daily_activity
bellaFit$daily_activity %>% select(calories, total_distance) %>%
        
        # Mapping the variables to the aesthetics of the plot using the aes() function 
        ggplot(mapping = aes(x = total_distance, y = calories)) +
        
        # Adding a layer to the plot to display points using the geom_point() function with transparency set to 0.5
        geom_point(alpha = 0.5) +  
        
        # Adding a layer to the plot to display a smoothed line using the geom_smooth() function, colored in red
        geom_smooth(col = "red", method = "loess") +
        
        # Customize the plot 
        labs(y = "Calories", x = "Total Distance")
```

3.  We can identify the kind of user according to the average steps per day using `dplyr`. In order to to compare the users' daily steps to an activity level, I considered the following categories: [How many steps should you take a day?](https://www.healthline.com/health/how-many-steps-a-day#How-many-steps-should-you-take-a-day?)

-   Less than 5000 steps: Inactive

-   Between 5000 and 10,000 steps: Average

-   More than 10,000 steps: Active

```{r User type steps }
# Selecting the columns "id" and "step_total" from the data frame bellaFit$daily_steps
bellaFit$daily_steps %>% select(id, step_total) %>%
        
        # Grouping the data and computing the mean number of steps per day for each id
        group_by(id) %>% summarise(mean_day_steps = mean(step_total)) %>%
        
        # Adding a new variable "category" based on the mean number of steps 
        mutate(category = factor(case_when(mean_day_steps < 5000 ~ "Inactive",
                                           mean_day_steps >= 5000 & mean_day_steps <= 10000 ~ "Average",
                                           mean_day_steps > 10000 ~ "Active"),
                                 levels = c("Inactive", "Average", "Active"),
                                 ordered = TRUE)) %>% 
        
        # Selecting only the "category" column using select() function and grouping the data by "category"
        select(category) %>% group_by(category) %>% 
        
        # Computing the count and percent of observations in each category
        summarise(count_category = n()) %>% mutate(percent = count_category / sum(count_category))
```

4.  Similarly, we can identify the kind of user according to the number of day they use their device using `dplyr`.

```{r User type daily use}
# Selecting the columns "id"  from the data frame bellaFit$daily_activity
bellaFit$daily_activity %>% select(id) %>%
        
        # Grouping the data and computing the count of id
        group_by(id) %>% summarise(days_used = n()) %>%
        
        # Adding a new variable "category" based on the number of days an id appears
        mutate(category = factor(case_when(days_used < 10 ~ "Low Use",
                                           days_used >= 10 & days_used <= 20 ~ "Average Use",
                                           days_used > 20 ~ "High Use"),
                                 levels = c("Low Use", "Average Use", "High Use"),
                                 ordered = TRUE)) %>%
        
        # Selecting only the "category" column using select() function and grouping the data by "category"
        select(category) %>% group_by(category) %>% summarise(count_category = n()) %>%
        
        # Computing the count and percent of observations in each category
        mutate(percent = count_category / sum(count_category))

```
