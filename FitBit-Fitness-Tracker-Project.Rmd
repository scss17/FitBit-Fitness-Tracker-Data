---
title: "FitBit-Fitness-Tracker-Project"
author: "PS"
date: '2023-03-10'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

## About the Project

**Business Task:** The aim of this project is to gain insights into smart device usage trends and their potential application to Bellabeat customers, as well as to use these findings to inform Bellabeat's marketing strategy. The analysis considers the following questions:

-   What are some trends in smart device usage?

-   How could these trends apply to Bellabeat customers?

-   How could these trends help influence Bellabeat marketing strategy

## About the Company

**Urška Sršen** and **Sando Mur** founded Bellabeat, a high-tech company that manufactures health-focused smart products. Collecting data on activity, sleep, stress, and reproductive health has allowed Bellabeat to empower women with knowledge about their own health and habits. Since it was founded in 2013, Bellabeat has grown rapidly and quickly positioned itself as a tech-driven wellness company for women.

By 2016, Bellabeat had opened offices around the world and launched multiple products. Bellabeat products became available through a growing number of online retailers in addition to their own e-commerce channel on their [website](https://bellabeat.com/).

**Sršen** knows that an analysis of Bellabeat's available consumer data would reveal more opportunities for growth. She has asked the marketing analytics team to focus on a Bellabeat product and analyze smart device usage data in order to gain insight into how people are already using their smart devices. Then, using this information, she would like high-level recommendations for how these trends can inform Bellabeat marketing strategy.

## About the Data

**Data Used**: The data set consisted of public data that explores smart device users' daily habits.

-   [FitBit Fitness Tracker Data](https://www.kaggle.com/datasets/arashnic/fitbit) (CC0: Public Domain, dataset made available through [Mobius](https://www.kaggle.com/arashnic)): This Kaggle data set contains personal fitness tracker from thirty Fitbit users. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring. It includes information about daily activity, steps, and heart rate that can be used to explore users' habits.

**Data Collection:** This dataset generated by respondents to a distributed **survey** via Amazon Mechanical Turk between 03.12.2016-05.12.2016. Thirty eligible Fitbit users consented to the submission of personal tracker data, including minute-level output for physical activity, heart rate, and sleep monitoring.

-   **Second-Party Data**: [Amazon Mechanical Turk (MTurk)](https://www.mturk.com/) is a crowdsourcing marketplace that makes it easier for individuals and businesses to outsource their processes and jobs to a distributed workforce who can perform these tasks virtually. This enables organizations to post tasks or surveys and pay people to complete them. Since the survey was subject to privacy regulations and data protection laws the assumption that the data is second-party is made. **Second-party data** refers to data that is collected by one organization or entity and then shared with another organization or entity.
-   **Potential Selection Bias**: This bias occurs when individuals choose whether or not to participate in a study or survey, and this choice is related to the outcome being studied. In this case, since 30 user consented to track their data, those individuals may differ in important ways from those who do not consent to participate and may not be a representative sample of the entire population of users.

**Data Organization**: The entire data consisted of 18 tables of CSV files. Individual reports can be parsed by export session ID or timestamp. Variation between output represents use of different types of Fitbit trackers and individual tracking behaviors / preferences.

-   Every file has an ID column which is a unique identifier for each user. We can use this variable to identify how many users are considered in each table.

-   I defined a function in order to get the number of users, variables and data types of each column within each table. The variable that represents the `datestamp` is parsed as `character`. This can cause issues when working with dates, therefore converting the data type from `character` to `Date` is required. Only the table `weightLogInfo` has a column with `logicals`. You may refer to the function in the documentation for further details.

-   The minimum first date in the `ActivityDate` is April 12th, 2016, while the last is May 12th of the same year. This provides a 30-day reporting period to analyze. It's possible that users' behavior during this 30 days may be influenced by factors that are specific to that period, such as a holiday or a special event. Such a small period might not be enough to generalize the users' behavior in the long run; however, it can still provide valuable insights into users' behavior during that time.

| Table Name              | Type     | Participants | Description                                            |
|:-----------------|:-----------------|:-----------------|:-------------------|
| dailyActivity           | CSV File | 33 users     | 15 variables: character 01 \| numeric 14               |
| dailyCalories           | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| dailyIntensities        | CSV File | 33 users     | 10 variables: character 01 \| numeric 09               |
| dailySteps              | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| heartrate               | CSV File | 14 users     | 03 variables: character 01 \| numeric 02               |
| hourlyCalories          | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| hourlyIntensities       | CSV File | 33 users     | 04 variables: character 01 \| numeric 03               |
| hourlySteps             | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minuteCaloriesNarrow    | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minuteCaloriesWide      | CSV File | 33 users     | 62 variables: character 01 \| numeric 61               |
| minuteIntensitiesNarrow | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minuteIntensitiesWide   | CSV File | 33 users     | 62 variables: character 01 \| numeric 61               |
| minuteMETsNarrow        | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minutesSleep            | CSV File | 24 users     | 04 variables: character 01 \| numeric 03               |
| minutesStepsNarrow      | CSV File | 33 users     | 03 variables: character 01 \| numeric 02               |
| minutesStepsWide        | CSV File | 33 users     | 62 variables: character 01 \| numeric 61               |
| sleepDay                | CSV File | 24 users     | 05 variables: character 01 \| numeric 04               |
| weightLogInfo           | CSV File | 08 users     | 08 variables: character 01 \| numeric 06 \| logical 01 |

------------------------------------------------------------------------

# Process

I will focus on conducting the analysis by utilizing the programming language R. R has a vast collection of packages and libraries that provide powerful and flexible tools for data analysis, visualization, and modeling, and besides that, is free open-source software. I will be using the following packages:

-   `ggplot2`: A package for creating visually appealing and informative data visualizations. It is based on the grammar of graphics, which allows users to build complex plots by combining simple components.

-   `readr`: A package for reading in and parsing flat files (e.g. CSV files) into R data frames. It is designed to be fast and memory-efficient, making it ideal for working with large data sets.

-   `dplyr`: A package for data manipulation and transformation. It provides a set of easy-to-use functions for filtering, grouping, summarizing, and joining data, making it a powerful tool for data wrangling tasks.

-   `lubridate`: A package that provides functions to work with dates and times. It makes it easier to parse, manipulate, and format date-time objects in R. The package provides a consistent and user-friendly syntax for common date-time operations, which can be difficult to perform in base R.

-   `janitor`: The janitor package is an R package that provides a set of functions for data cleaning and data tidying tasks. It offers a range of simple and consistent functions that can help clean up messy datasets and quickly perform common data manipulation tasks.

```{r Load libraries, message = FALSE, warning = FALSE}
# Load libraries
library(ggplot2)
library(readr)
library(dplyr)
library(lubridate)
library(janitor)
```

As mentioned before, the entire data consisted of 18 tables of CSV files. The task of importing the data into R using `read_csv` could be cumbersome if we proceeded manually. Instead, I will create a function to do that task.

-   This function will import a list of CSV files located in a specified folder and returns them as a named list of data frames in R.

-   The original table names contain an underscore (`_`) somewhere in the string, I am going to use that underscore to create shorter names for each table in such a way that: `dailyActivity_merged.csv` will be `dailyActivity`, for instance.

-   In the end, this function will be useful for importing multiple CSV files into R as a single named list of data frames and, I will use that list to analyse their data type. The advantage of having a list of data frames instead of individual data frames is that it facilitates the management of multiple datasets. Rather than keeping track of individual data frames separately, a list can store and manipulate them as a single object. This can simplify the code and make it easier to apply functions or manipulate the data in a consistent manner across all datasets.

```{r Import data sets, message = FALSE}
# Create a function to import all the data sets within the folder as a list
importDataList <- function(folder_path = "data") {
        
        # Get a list off all files names in the given folder
        file_names <- list.files(path = folder_path)
        
        # Create empty vectors to store data names and data lists
        data_name <- c()
        data_list <- list()
        
        # Loop through each file in the list of file names
        for(i in seq_along(file_names)) {
                
                # Find the position of the first underscore in the file 
                char_pos <- sapply(lapply(strsplit(file_names, split = ""), 
                                          function(x) {which(x == "_")}), 
                                   function(x) {x[1]})
                
                # Extract the data name from the file name 
                data_name[i] <- substr(file_names[i], start = 1, stop = char_pos[i] - 1)
                
                # Read in the CSV file and store it in the data list
                file_path <- paste0(folder_path, "/", file_names[i])
                data_list[[i]] <- read_csv(file_path)
        }
        
        # Rename the data sets in the list and return the list
        names(data_list) <- data_name
        return(data_list)
}
bellaFit <- importDataList()
```

Once the data set is loaded, I need to change some variables.

1.  The variables that represent the `datestamp` are parsed as `character`. Therefore, for converting the data type from character to Date I use the following chuck of code. For instance, in `dailyActivity` data frame, the column is modified using the existing `ActivityDate` column, but with its format converted to date using the `lubridate::as_date` function. The specified format for the date conversion is `"%m/%d/%Y"`, which means that the original date strings are expected to be in the format of `month/day/year`.

```{r Parsing character as date}
# Change data type from text to date in dailyActivity data set
bellaFit$dailyActivity <- bellaFit$dailyActivity %>%
        mutate(ActivityDate = as_date(ActivityDate, format = "%m/%d/%Y"))

# Change data type from text to date in dailySteps data set
bellaFit$dailySteps <- bellaFit$dailySteps %>%
        mutate(ActivityDay = as_date(ActivityDay, format = "%m/%d/%Y"))

# Change data type from text to date in sleepDay data set
bellaFit$sleepDay <- bellaFit$sleepDay %>%
        mutate(SleepDay = as_date(SleepDay, format = "%m/%d/%Y"))
```

2.  The column names also need to be changed. I use **`clean_names()`** function from the **`janitor`** package to clean up the column names of a data frame. In this code, the **`clean_names()`** is used to standardize the column names of three data frames within the **`bellaFit`** object: **`dailyActivity`**, **`dailySteps`**, and **`sleepDay`**.
3.  For the sake of consistency, we rename the data sets with this standardize convention as well.

```{r Rename column names}
# Clean up the column names of the dailyActivity data frame 
bellaFit$dailyActivity <- clean_names(bellaFit$dailyActivity)

# Clean up the column names of the dailySteps data frame
bellaFit$dailySteps <- clean_names(bellaFit$dailySteps)

# Clean up the column names of the sleepDay data frame 
bellaFit$sleepDay <- clean_names(bellaFit$sleepDay)

# Rename data set names
names(bellaFit)[which(names(bellaFit) %in% c("dailyActivity", "dailySteps", "sleepDay"))] <- c("daily_activity", "daily_steps", "sleep_day")  
```

4.  We need to ensure that there are no duplicate rows in the data. Removing duplicated rows is an important step in data cleaning because duplicated data can bias statistical analysis and lead to incorrect results. Then I removed the duplicated rows afterwards.

```{r Remove duplicated rows}
# Create a data frame that lists the number of duplicated rows in three tables of a larger data set
data.frame(
        
        # Column that lists the name of each table
        table_name = c("daily_activity", "daily_steps", "sleep_day"),
        
        # Column that lists the number of duplicated rows in each table
        duplicated_rows = c(
                
                # Calculate the number of duplicated rows
                sum(duplicated(bellaFit$daily_activity)), # daily_activity table
                sum(duplicated(bellaFit$daily_steps)), # daily_steps table
                sum(duplicated(bellaFit$sleep_day)) # sleep_day table
        )
) 

# Remote duplicated rows
bellaFit$sleep_day <- bellaFit$sleep_day %>% distinct()
```

# Analyze 
